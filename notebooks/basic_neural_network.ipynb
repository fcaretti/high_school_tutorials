{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benvenuto, iniziamo!\n",
    "\n",
    "Questo √® un **Notebook in Python**, cio√® uno strumento interattivo per scrivere ed eseguire del codice in modo dinamico. √à composto da *celle* che possono contenere testo (come questa) oppure codice Python (come la prossima).\n",
    "\n",
    "Per *eseguire* i comandi in una cella, puoi:\n",
    "- cliccare sul simbolo \"Play\" a sinistra della cella, oppure\n",
    "- selezionare la cella e premere Shift + Invio sulla tastiera\n",
    "\n",
    "Se non √® gi√† stato selezionato, al primo avvio Visual Studio potrebbe chiederti con quale *kernel* eseguire il codice (in pratica... con quale \"versione\" di Python): tu seleziona *Python Environments* e poi *env*. Abbiamo configurato noi l'ambiente cos√¨ da farti trovare tutte le librerie necessarie gi√† pronte per l'uso!\n",
    "\n",
    "Perch√© tutto funzioni come previsto, ricordati di eseguire ogni cella di codice prima di passare alla successiva. Le variabili, funzioni e classi definite in una cella sono utilizzabili in tutto il notebook, una volta che la cella viene eseguita.\n",
    "\n",
    "Per il resto, √® il solito Python. Buon lavoro! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie necessarie\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch # questa √® una libreria molto usata per le Reti Neurali\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Il problema: riconoscimento immagini\n",
    "Il tuo obiettivo sar√† seguire passo dopo passo (e personalizzare) la costruzione e l'addestramento di una rete neurale per classificare un certo tipo di immagini. Queste immagini saranno di dimensione 28x28 pixel e rappresenteranno **numeri da 0 a 9 scritti a mano** (quindi con grafie diverse tra loro). Noi umani siamo ben in grado di riconoscere scritture differenti... Vorremmo che la rete neurale fosse in grado di fare la stessa cosa!\n",
    "\n",
    "Ingredienti:\n",
    "- i dati (immagini dei numeri)\n",
    "- il modello (rete neurale)\n",
    "- l'allenamento (processo con cui la rete impara a riconoscere i numeri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) I dati\n",
    "La libreria PyTorch offre la possibilit√† di scaricare i dataset pi√π usati con grande facilit√†.\n",
    "\n",
    "Ci servono:\n",
    "- un insieme di dati per allenare il modello (*train set*) \n",
    "- un insieme di dati (diversi) per testarlo (*test set*).\n",
    "\n",
    "Secondo te, come mai √® importante testare il modello su esempi diversi da quelli su cui √® allenato?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applichiamo la trasformazione ToTensor() per convertire le immagini in tensori (sono una specie di vettori)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Importiamo e scarichiamo i dati di train e di test\n",
    "train_set = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Creiamo un dataloader per il train e uno per il test\n",
    "# questo tipo di oggetto permette in modo efficiente di gestire e caricare i dati in PyTorch...\n",
    "# dal nostro punto di vista √® importante solo sapere che questi oggetti contengono i dati che useremo nel nostro modello!\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=len(test_set), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta scaricati i dati, possiamo visualizzarne alcuni per capire come sono fatti! Prendiamo 5 esempi dal train set e stampiamoli a video con la libreria *matplotlib*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    # Visualizziamo cosa c'√® nel train set... \n",
    "    # train_set √® una lista di coppie\n",
    "    # train_set[i][0] √® l'immagine, mentre train_set[i][1] √® il numero rappresentato nell'immagine (che chiamiamo \"label\")\n",
    "\n",
    "    image, label = train_set[i]  # Otteniamo l'immagine e il label\n",
    "\n",
    "    # Ci serve fare questo passaggio per convertire l'immagine in un oggetto visualizzabile con matplotlib (importata come plt)\n",
    "    imago_to_plot = image.numpy().squeeze()\n",
    "\n",
    "    ax[i].imshow(imago_to_plot, cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "\n",
    "print('I numeri sono:', [train_set[i][1] for i in range(5)])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Il modello\n",
    "Il modello che useremo sar√† una rete neurale molto di base. Ripassiamo brevemente come sono fatte.\n",
    "Le reti sono composte da neuroni, disposti in strati (*layers*), ognuno dei quali pu√≤ contenere un numero variabile di neuroni. Il numero di strati √® detto profondit√† (*depth*) della rete.\n",
    "\n",
    "Nel modello che vedremo oggi, ogni neurone di un layer manda un segnale ad ogni neurone del layer successivo. La forza della connessione tra due neuroni √® determinata da un peso ed √® proprio questo peso quello che viene allenato. Quindi la rete, grazie all'allenamento, riuscir√† a capire quale sia il peso migliore da usare in ogni connessione affinch√® le immagini siano riconosciute correttamente!\n",
    "\n",
    "Quello che succede in ciasun neurone √® questo:\n",
    "- riceve un'informazione da tutti i neuroni del layer precedente\n",
    "- pesa i singoli input e li combina tra loro (facendo una **somma pesata**, ovvero una somma in cui ogni addendo √® moltiplicato per un fattore, il peso)\n",
    "- applica una cosidetta **funzione di attivazione** alla somma ottenuta\n",
    "- manda il valore che ne risulta ad ogni neurone del layer successivo\n",
    "\n",
    "Alla fine della rete, ci sar√† un layer con un numero di neuroni pari al numero di classi in cui vogliamo classificare i nostri dati. Nel nostro caso saranno 10, perch√© stiamo cercando di riconoscere le cifre da 0 a 9.\n",
    "\n",
    "#### Definiamo la rete neurale in Python\n",
    "Per costruire una rete neurale ci serve usare la classe *SimpleNeuralNetwork*. Una classe in Python √® un tipo di struttura che ti permette di creare nuovi oggetti con caratteristiche e comportamenti specifici (ad esempio, le liste di Python hanno la loro struttura e loro metodi, come *.append()*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La rete neurale sar√† un oggetto della classe SimpleNeuralNetwork, che definiamo noi\n",
    "class SimpleNeuralNetwork(torch.nn.Module):\n",
    "\n",
    "    # Definiamo la funzione per inizializzare una rete, ci serve sapere:\n",
    "    # - depth, il numero di strati (layer) della rete\n",
    "    # - width, il numero di neuroni in ogni singolo strato\n",
    "\n",
    "    def __init__(self, depth, width):\n",
    "        super().__init__() # questa riga fa \"ereditare\" automaticamente alla nostra classe alcune funzioni standard delle reti neurali\n",
    "        self.layers = [] # creiamo la lista degli strati della rete, intanto vuota\n",
    "        if depth <= 1:\n",
    "            raise ValueError('depth deve essere almeno 1')\n",
    "        \n",
    "        # La lista layer avr√† due elementi per ogni strato:\n",
    "        # - un oggetto linear (per fare la somma pesata)\n",
    "        # - un oggetto relu (la funzione di attivazione)\n",
    "        self.layers.append(torch.nn.Linear(28*28, width)) # questo √® il layer iniziale\n",
    "                                                          # i primi neuroni riceveranno le immagini di 28x28 pixel del train set\n",
    "        self.layers.append(torch.nn.ReLU())\n",
    "\n",
    "        # Gli strati in mezzo sono tutti uguali (gli unici diversi sono il primo e l'ultimo)\n",
    "        # quindi li definiamo con un ciclo for\n",
    "        for i in range(depth-2):\n",
    "            self.layers.append(torch.nn.Linear(width, width))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # L'ultimo strato deve essere di 10 classi\n",
    "        self.head = torch.nn.Linear(width, 10)\n",
    "    \n",
    "    # Definiamo la funzione che dice come far passare i valori tra un layer e l'altro\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambia a piacere questi valori, basta che siano interi positivi (e che la depth sia almeno 2)\n",
    "depth = 3\n",
    "width = 16\n",
    "model = SimpleNeuralNetwork(depth, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) La strategia di allenamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In modo simile (pi√π o meno) a come le connessioni tra i nostri neuroni cambiano nel tempo per farci diventare pi√π bravi nelle attivit√† che facciamo, vogliamo cambiare i pesi tra le coppie di neuroni, che inizialmente sono casuali, in modo che la rete diventi brava a svolgere il compito che le assegniamo: cio√® classificare le immagini capendo che numero c'√® disegnato.\n",
    "\n",
    "Per farlo, ci serve calcolare quanti errori di etichettamento fa la rete, dato un certo set di pesi. Definiamo, quindi, una funzione che chiamiamo *loss function* (o *cost function*), che tiene conto di quanto spesso la rete sbaglia a classificare le immagini del dataset. Se sbaglia molto spesso, la loss function sar√† alta. Man mano che alleniamo la rete e questa inizia a classificare correttamente pi√π immagini, vedremo che la loss function diventer√† sempre pi√π bassa.\n",
    "\n",
    "Il nostro \"allenamento\" si propone proprio di cambiare i pesi della rete in modo da abbassare sempre di pi√π il valore della loss function, e quindi di classificare correttamente sempre pi√π immagini.\n",
    "\n",
    "Calcoliamo la loss function prima di iniziare l'allenamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendiamo la funzione gi√† definita\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# \"Carichiamo\" i dati\n",
    "x, y = next(iter(test_loader))  \n",
    "# Facciamo passare le immagini dentro al modello; y_pred contiene le predizioni per ogni immagine\n",
    "y_pred = model(x)  \n",
    "\n",
    "# Calcoliamo la loss confrontando le classificazioni corrette, gi√† presenti nei dati, che abbiamo salvato nella variabile y, \n",
    "# con le classificazioni predette dal modello con i pesi attuali, salvate in y_pred\n",
    "loss = loss_function(y_pred, y)\n",
    "\n",
    "print('Loss prima dell\\'allenamento:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a visualizzare degli esempi per capire se il nostro modello √® gi√† buono o meno. Vediamo degli esempi di immagini e del numero in cui viene classificata ciascuna dal modello.\n",
    "\n",
    "**Completa tu il codice, dove indicato nel commento.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo alcuni esempi di immagini in una griglia 5x5\n",
    "\n",
    "# plt.subplots crea una figura con una griglia di 5x5 immagini:\n",
    "# - la variabile ax √® una matrice (lista di liste), dove ogni elemento ax[j][i] (per i = 0,...4 e j = 0,...4) \n",
    "#   √® un singolo riquadro della griglia in cui possiamo disegnare\n",
    "fig, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
    "\n",
    "# Loop su una griglia 5x5\n",
    "for j in range(5):\n",
    "    for i in range(5):\n",
    "        # Da i e j ricavo un numero tra 0 e 24 ... che sar√† il numero dell'esempio\n",
    "        example_number = 5*j + i\n",
    "\n",
    "        # Prima visualizzo le immagini di test...\n",
    "        image, label = test_set[example_number]\n",
    "        # image.numpy().squeeze() converte un oggetto adatto a lavorare con le reti neurali in una immagine visualizzabile\n",
    "        ax[j][i].imshow(image.numpy().squeeze(), cmap='gray')\n",
    "        ax[j][i].axis('off')\n",
    "\n",
    "        # ... Poi visualizzo i numeri associati dalla rete neurale\n",
    "        pred_label = str(torch.argmax(y_pred[5*j + i]).item())\n",
    "        # Aggiungi alla figura in ax[j][i] un titolo usando \"pred_label\" (numero \"predetto\") definito sopra:\n",
    "        # in questo modo sopra ad ogni immagine comparir√† il numero associato dalla rete neurale!\n",
    "\n",
    "        ############# SCRIVI QUI IL TUO CODICE #############\n",
    "\n",
    "# Mostriamo la figura\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello *d√† i numeri* (pun intended), ma sono sbagliati. Questo √® perch√© non √® ancora stato allenato, ovvero le connessioni tra coppie di neuroni sono dei numeri casuali. Ora vedremo come migliorarle.\n",
    "\n",
    "La strategia pi√π usata consiste nel calcolare la direzione in cui la loss function \"scende\" e fare un passetto in quella, usando alcuni dati (questo significa modificare leggermente i pesi in base al valore di questa \"direzione\", in modo da diminuire la loss). Dopodich√©, si ricalcola la loss, si ricalcola la direzione giusta, si fa un altro passetto, e cos√¨ via.\n",
    "\n",
    "Questo algoritmo si chiama *Stochastic Gradient Descent*, di solito abbreviato *SGD*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](https://www.researchgate.net/profile/Bhaskar-Ghosh/publication/344544069/figure/fig4/AS:944366457729024@1602165917164/Stochastic-Gradient-Descent.ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per fare questo apprendimento, usiamo la funzione \"optimizer\" dalla solita libreria; \n",
    "# il parametro lr √® il learning rate, ovvero l'ampiezza del passo \n",
    "# il lr regola di quanto vengono aggiornati i pesi della rete ad ogni step\n",
    "# momentum √® un parametro che memorizza la direzione degli step precedenti e aiuta a continuare a seguire una direzione simile\\\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso proviamo a fare uno di questi passi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendiamo i pesi dell'ultimo layer prima del passo\n",
    "weights_before = model.head.weight.clone()\n",
    "\n",
    "# Ci serve fare un ciclo sul \"caricatore\" di dati, che automaticamente ne prende un po' scelti a caso\n",
    "for images, labels in train_loader:   \n",
    "    optimizer.zero_grad()  # Questo bisogna metterlo, prendilo come una specie di reset\n",
    "    outputs = model(images)  # Mettiamo le immagini dentro al modello e ci prendiamo le predizioni\n",
    "    loss = loss_function(outputs, labels)  # Calcoliamo la loss function\n",
    "    loss.backward()  # Questo comando calcola la direzione del passo da fare per ogni coppia di neuroni connessi della rete\n",
    "    optimizer.step()  # La forza della connessione dei neuroni viene aggiornata\n",
    "    break\n",
    "\n",
    "# Prendiamo i pesi dell'ultimo layer dopo il passo\n",
    "weights_after = model.head.weight.clone()\n",
    "\n",
    "# Stampiamo i pesi della connessione tra il primo neurone del penultimo layer e i neuroni dell'ultimo layer\n",
    "# Dovremmo vederli cambiare, anche se di poco, visto che abbiamo fatto un solo passetto\n",
    "print(weights_before[0])\n",
    "print(weights_after[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalmente, possiamo allenare la rete neurale\n",
    "Ora abbiamo tutti gli ingredienti per allenare la rete neurale.\n",
    "\n",
    "La procedura di allenamento consiste semplicemente nel ripetere tante volte il passetto di prima. Per farlo togliamo il `break` messo all'interno del loop `for images, labels in train_loader` del codice sopra. In questo modo non fermiamo il loop al primo step, ma facciamo un giro su tutto il train set. Uno di questi giri si chiama *epoca*. Se non siamo soddisfatti del livello raggiunto dopo un'epoca, possiamo fare altri giri (ma ovviamente questo richiede pi√π tempo!)\n",
    "\n",
    "Nella cella qui sotto, riempi lo spazio per il singolo passetto come fatto sopra. Inoltre, per capire quanto bene la rete sta facendo, sarebbe comodo definire una funzione che calcoli l'*accuracy* (il numero di immagini che riesce a classificare correttamente, diviso per il totale delle immagini). Trovi la funzione sotto da completare come extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 5  # Numero di epoche: se vuoi, puoi provare a modificare questo valore!\n",
    "test_losses = []\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)  \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        \n",
    "    # Test del modello: ci interessa che la loss function scenda sul test set\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            test_loss = loss_function(outputs, labels)\n",
    "            #accuracy = calculate_accuracy(outputs, labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    #print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come prima, visualizziamo alcune immagini insieme al numero che ci d√† la rete. Magari non saranno tutti giusti, ma dovresti osservare un grande miglioramento rispetto a prima!\n",
    "\n",
    "In generale, avere una rete pi√π grande (ovvero con pi√π neuroni per layer e con pi√π layer) e allenarla di pi√π dovrebbe aiutare.\n",
    "Puoi provare a tornare alla cella in cui abbiamo definito il numero di neuroni e aumentarlo, oppure aumentare il numero di epoche, se non ti soddisfa il risultato raggiunto. Ricorda che, dopo aver modificato, dovrai eseguire nuovamente e in ordinatamente tutte le celle che ci stanno sotto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
    "\n",
    "# Loop su una griglia 5x5\n",
    "for j in range(5):\n",
    "    for i in range(5):\n",
    "        # Da i e j ricavo un numero tra 0 e 24 ... che sar√† il numero dell'esempio\n",
    "        example_number = 5*j + i\n",
    "\n",
    "        # Prima visualizzo le immagini...\n",
    "        image, label = test_set[example_number]\n",
    "        outputs = model(image) # TODO commentare qui cosa fa \n",
    "        # image.numpy().squeeze() converte un oggetto adatto a lavorare con le reti neurali in una immagine visualizzabile\n",
    "        ax[j][i].imshow(image.numpy().squeeze(), cmap='gray')  # Convertiamo il tensore in NumPy\n",
    "        ax[j][i].axis('off')\n",
    "\n",
    "        # ... Poi visualizzo i numeri associati dalla rete neurale\n",
    "        pred_label = str(torch.argmax(outputs).item())\n",
    "        # Aggiungi alla figura in ax[j][i] un titolo usando \"pred_label\" definito sopra:\n",
    "        # in questo modo sopra ad ogni immagine comparir√† il numero associato dalla rete neurale!\n",
    "\n",
    "        ############# SCRIVI QUI IL TUO CODICE #############\n",
    "        ax[j][i].set_title(pred_label, fontsize=10, color='blue') # TODO cancellare\n",
    "\n",
    "\n",
    "# Mostriamo la figura\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: calcoliamo l'accuratezza della rete\n",
    "\n",
    "Fin qui, abbiamo detto che non ci interessa esattamente il valore della loss function, solo che scenda; tuttavia, potrebbe essere utile calcolare una quantit√† con un significato ben preciso per valutare quanto bene la rete sta facendo: l'accuracy (accuratezza), ovvero il numero di esempi che indovina diviso per il numero totale di esempi.\n",
    "\n",
    "Potresti chiederti come mai non si possa usare direttamente questa come loss function (o meglio, $1 - accuracy$): il motivo √® che per calcolare la direzione dello step di allenamento, la loss function deve essere *continua*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, labels):\n",
    "    # Outputs √® una matrice (=una tabella) con le probabilit√† di appartenenza ad ogni classe; √® di dimensione (numero di immagini, numero di classi)\n",
    "    # Per ottenere la classe predetta, prendiamo l'indice della colonna con probabilit√† massima\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    # Ora predictions contiene il massimo di ogni riga, ovvero la classe predetta per ogni immagine\n",
    "\n",
    "    # Scrivi qui il codice per calcolare l'accuracy\n",
    "    # correct = ...\n",
    "    # total = ...\n",
    "    # return correct / total\n",
    "\n",
    "# Testiamo la funzione\n",
    "# a √® una matrice con le probabilit√† di appartenenza a ciascuna classe; ci sta dicendo che il modello prevede che la prima immagine sia della classe 2 e la seconda della classe 0\n",
    "# Ricordati che in Python, le liste e i vettori partono da 0 e non da 1!\n",
    "a = torch.tensor([[0.1, 0.2, 0.7], [0.3, 0.3, 0.4]])\n",
    "# b invece contiene direttamente la soluzione corretta\n",
    "b = torch.tensor([2, 0])\n",
    "\n",
    "# Il risultato giusto dovrebbe essere 1/2 = 0.5\n",
    "print(calculate_accuracy(a, b))\n",
    "\n",
    "# Se funziona, puoi tornare alla cella in cui abbiamo allenato il modello e togliere il commento alla riga che calcola l'accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se sei curioso... Qualche approfondimento\n",
    "\n",
    "Un aspetto fondamentale affinch√© una rete neurale funzioni √® che la funzione di attivazione deve essere **non lineare**. Non lineare che significa? Praticamente, non deve essere una retta...\n",
    "\n",
    "Se cos√¨ non fosse, la rete si comporterebbe esattamente come una rete con un solo strato (*layer*), perdendo la capacit√† di modellare relazioni complesse nei dati.\n",
    "\n",
    "La cella qui sotto di permette di visualizzare le due funzioni di attivazione pi√π usate: la sigmoide e la ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "y_sigmoid = 1 / (1 + np.exp(-x))\n",
    "y_relu = np.maximum(0, x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(x, y_sigmoid)\n",
    "ax[0].set_title('Sigmoide')\n",
    "ax[1].plot(x, y_relu)\n",
    "ax[1].set_title('ReLU')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s4s_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
